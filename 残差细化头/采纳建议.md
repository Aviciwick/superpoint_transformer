这是一份关于**模块三：残差细化头 (Residual Refinement Head, RRH)** 两个设计方案（我们称之为 **Design A** 和 **Design B**）的详细对比评判。

这两份方案虽然在核心算法逻辑（广播-拼接-MLP）上保持一致，但在**工程鲁棒性**、**网络结构细节**以及**对残差学习的定义**上存在微妙但关键的差异。

### 1. 核心差异对比矩阵

| 维度 | **Design A (概念原型)** [Sources 101-106] | **Design B (工程交付)** [Sources 215-221] |
| :--- | :--- | :--- |
| **残差策略** | **强制残差**。公式为 $Final = Coarse + \Delta$。将 SPT 的原始 Logits 作为 Base，强制网络只学习偏移量。 | **可选/混合策略**。代码支持传入 `sp_logits` 做残差，也支持不传（直接预测）。增加了灵活性。 |
| **网络结构** | **极简版**。`Linear -> ReLU -> Linear`。 | **增强版**。`Linear -> ReLU -> Dropout -> Linear`。引入了 **Dropout** 防止过拟合。 |
| **防御性编程** | **弱**。虽提及 $K=0$ 处理，但侧重于逻辑描述。 | **强**。明确要求代码检测 `K=0` 并直接返回空 Tensor，防止 `forward` 崩溃。 |
| **显存优化** | 建议在 Dataloader 切片 $N$。 | 建议使用 `inplace=True` 的 ReLU，并复用模块二特征。 |
| **角色定义** | **“外科医生”**。强调“打破超点边界 (Superpoint Breaking)”。 | **“边界重构者”**。强调“密集预测 (Dense Prediction)”。 |

---

### 2. 深度优劣势剖析

#### A. 残差学习的纯粹性 (Residual Learning Purity)
*   **Design A** [Source 105] 更贴合“Refinement（细化）”的第一性原理。它强制网络学习 $Logits_{final} = Logits_{superpoint} + \Delta Logits_{point}$。
    *   *优势*：数学上更容易收敛。因为大部分点（如墙面中心）的标签与超点一致，$\Delta$ 为 0 即可；网络只需要关注边界那些 $\Delta \neq 0$ 的点。
    *   *劣势*：如果 SPT 的 `coarse_logits` 本身错得离谱，强制加和可能会把错误放大。

*   **Design B** [Source 220] 提供了**条件分支**。
    *   *优势*：如果发现强制残差效果不好，可以通过把 `sp_logits` 设为 `None` 快速切换回“纯点级分类器”模式，方便消融实验（Ablation Study）。

#### B. 工程实现的落地性 (Engineering Robustness)
*   **Design B** [Source 219-220] 明显是经过工程思考的产物。
    *   **$K=0$ 保护**：在不确定性采样中，极端情况下可能没有点被选中。Design B 明确指出的 `if K=0 return empty` 是防止训练中断的救命稻草。
    *   **Dropout**：考虑到 Module 3 处理的是困难样本，且 MLP 参数量相对于样本量可能较大，Design B 引入 `Dropout` [Source 220] 是非常必要的正则化手段。

---

### 3. 最终融合建议 (The Hybrid Strategy)

为了给 AI Code Agent 提供最完美的指令，建议**以 Design B 的代码结构为骨架，注入 Design A 的残差逻辑核心**。

**最佳实践方案：**

1.  **采用 Design B 的类结构与接口**：
    *   保留 `enhanced_sp_features` 和 `point_features_K` 的输入接口，确保与模块二对接。
    *   保留 `if hard_sp_indices.numel() == 0` 的**防御性检查**。
    *   保留 `Dropout` 层以提高泛化能力。

2.  **强制执行 Design A 的残差逻辑**：
    *   虽然 Design B 说是“可选”，但在实际实现时，应**默认开启** `sp_logits` 的输入，并执行加法操作。
    *   *理由*：这符合我们“H-SPT”课题的核心故事——我们在“修补”而非“重做”。

3.  **明确 Loss 计算位置（共识）**：
    *   两个方案都强调了 **Loss 不在模块内计算** [Source 106, 217]。这一点必须在给 AI 的 Prompt 中加粗强调，否则 AI 习惯性地会在 forward 里算 Loss，导致代码耦合度过高。

### 4. 给 AI Code Agent 的最终指令摘要

> "请采用 **Design B** 的代码架构（包含 Dropout 和 K=0 防御性检查），但在核心计算逻辑上，**强制要求**输入 `sp_logits` 并实现 **Design A** 定义的 $Logits + \Delta$ 残差加法。同时，请务必使用 `inplace=True` 的激活函数以优化显存。"